<!DOCTYPE html>
<html style="font-size: 16px;">
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="utf-8">
    <meta name="keywords" content="">
    <meta name="description" content="">
    <meta name="page_type" content="np-template-header-footer-from-plugin">
    <title>Human-Robot-AI</title>
    <link rel="stylesheet" href="nicepage.css" media="screen">
<link rel="stylesheet" href="Human-Robot-AI.css" media="screen">
    <script class="u-script" type="text/javascript" src="jquery.js" defer=""></script>
    <script class="u-script" type="text/javascript" src="nicepage.js" defer=""></script>
    <meta name="generator" content="Nicepage 4.8.2, nicepage.com">
    <link id="u-theme-google-font" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i|Open+Sans:300,300i,400,400i,500,500i,600,600i,700,700i,800,800i">
    
    
    
    <script type="application/ld+json">{
		"@context": "http://schema.org",
		"@type": "Organization",
		"name": "yhwang",
		"logo": "images/HKUST-logo.png"
}</script>
    <meta name="theme-color" content="#478ac9">
    <meta property="og:title" content="Human-Robot-AI">
    <meta property="og:description" content="">
    <meta property="og:type" content="website">
  </head>
  <body class="u-body u-xl-mode"><header class="u-clearfix u-grey-5 u-header u-sticky u-sticky-e2cb u-header" id="sec-63bd"><div class="u-clearfix u-sheet u-sheet-1">
        <a href="https://hkust.edu.hk" class="u-align-left u-image u-logo u-image-1" data-image-width="1280" data-image-height="410" title="HKUST" target="_blank">
          <img src="images/HKUST-logo.png" class="u-logo-image u-logo-image-1">
        </a>
        <nav class="u-menu u-menu-dropdown u-offcanvas u-menu-1">
          <div class="menu-collapse" style="font-size: 1rem; letter-spacing: 0px; font-weight: 400;">
            <a class="u-button-style u-custom-left-right-menu-spacing u-custom-padding-bottom u-custom-text-active-color u-custom-text-hover-color u-custom-top-bottom-menu-spacing u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="#">
              <svg class="u-svg-link" viewBox="0 0 24 24"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#menu-hamburger"></use></svg>
              <svg class="u-svg-content" version="1.1" id="menu-hamburger" viewBox="0 0 16 16" x="0px" y="0px" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg"><g><rect y="1" width="16" height="2"></rect><rect y="7" width="16" height="2"></rect><rect y="13" width="16" height="2"></rect>
</g></svg>
            </a>
          </div>
          <div class="u-custom-menu u-nav-container">
            <ul class="u-nav u-unstyled u-nav-1"><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-custom-color-3 u-text-hover-custom-color-2" href="Home.html" style="padding: 10px 20px;">Home</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-custom-color-3 u-text-hover-custom-color-2" href="About-Me.html" style="padding: 10px 20px;">About Me</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-custom-color-3 u-text-hover-custom-color-2" href="Students.html" style="padding: 10px 20px;">Students</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-custom-color-3 u-text-hover-custom-color-2" href="Awards.html" style="padding: 10px 20px;">Awards</a>
</li></ul>
          </div>
          <div class="u-custom-menu u-nav-container-collapse">
            <div class="u-black u-container-style u-inner-container-layout u-opacity u-opacity-95 u-sidenav">
              <div class="u-inner-container-layout u-sidenav-overflow">
                <div class="u-menu-close"></div>
                <ul class="u-align-center u-nav u-popupmenu-items u-unstyled u-nav-2"><li class="u-nav-item"><a class="u-button-style u-nav-link" href="Home.html">Home</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="About-Me.html">About Me</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="Students.html">Students</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="Awards.html">Awards</a>
</li></ul>
              </div>
            </div>
            <div class="u-black u-menu-overlay u-opacity u-opacity-70"></div>
          </div>
        </nav>
      </div></header>
    <section class="u-clearfix u-section-1" id="sec-ba31">
      <div class="u-clearfix u-sheet u-valign-bottom-xs u-sheet-1">
        <div class="u-expanded-width-lg u-expanded-width-md u-expanded-width-sm u-expanded-width-xl u-list u-list-1">
          <div class="u-repeater u-repeater-1">
            <div class="u-container-style u-custom-item u-list-item u-palette-5-light-3 u-repeater-item u-list-item-1">
              <div class="u-container-layout u-similar-container u-valign-bottom-sm u-container-layout-1">
                <h3 class="u-text u-text-1"><b> Deep convolutional neural network–based pixel-wise landslide inventory mapping</b>
                </h3>
                <img class="u-hidden-md u-hidden-sm u-hidden-xs u-image u-image-default u-image-1" src="images/WX20220415-2050462x-174.png" alt="" data-image-width="1230" data-image-height="1362">
                <p class="u-text u-text-2"> This paper reports a feasible alternative to compile a landslide inventory map (LIM) from remote sensing datasets using the application of an artificial intelligence–driven methodology. A deep convolutional neural network model, called LanDCNN, was developed to generate segmentation maps of landslides, and its performance was compared with the benchmark model, named U-Net, and other conventional object-based methods. The landslides that occurred in Lantau Island, Hong Kong, were taken as the case study, in which the pre- and post-landslide aerial images, and a rasterized digital terrain model (DTM) were used. The assessment reveals that LanDCNN trained with bitemporal images and DTM yields the smoothest and most semantically meaningfully LIM, compared to other methods. This LIM is the most balanced segmentation results, represented by the highest F1&nbsp;measure among all analyzed cases. With the encoding capability of LanDCNN, the application of DTM as the input renders better LIM production, especially when the landslide signatures are relatively subtle. With the computational setup used in this study, LanDCNN requires ~ 3&nbsp;min to map landslides from the datasets of approximately 25&nbsp;km2&nbsp;in area and with a resolution of 0.5&nbsp;m. In short, the proposed landslide mapping framework, featured LanDCNN, is scalable to handle the vast amount of remote sensing data from different types of measurements within a short processing period.</p>
              </div>
            </div>
            <div class="u-container-style u-custom-item u-list-item u-palette-5-light-3 u-repeater-item">
              <div class="u-container-layout u-similar-container u-valign-bottom-sm u-container-layout-2">
                <h3 class="u-text u-text-3"><b> DV-Det: Efficient 3D Point Cloud Object Detection with Dynamic Voxelization</b>
                </h3>
                <img class="u-hidden-md u-hidden-sm u-hidden-xs u-image u-image-default u-image-2" src="images/WX20220418-1232162x.png" alt="" data-image-width="1672" data-image-height="882">
                <p class="u-text u-text-4"> In this work, we propose a novel two-stage framework for the efficient 3D point
cloud object detection. Instead of transforming point clouds into 2D bird eye
view projections, we parse the raw point cloud data directly in the 3D space yet
achieve impressive efficiency and accuracy. To achieve this goal, we propose
dynamic voxelization, a method that voxellizes points at local scale on-the-fly. By
doing so, we preserve the point cloud geometry with 3D voxels, and therefore
waive the dependence on expensive MLPs to learn from point coordinates. On
the other hand, we inherently still follow the same processing pattern as point-
wise methods (e.g., PointNet) and no longer suffer from the quantization issue
like conventional convolutions. For further speed optimization, we propose the
grid-based downsampling and voxelization method, and provide different CUDA
implementations to accommodate to the discrepant requirements during training
and inference phases. We highlight our efficiency on KITTI 3D object detection
dataset with 75 FPS and on Waymo Open dataset with 25 FPS inference speed
with satisfactory accuracy.&nbsp;</p>
              </div>
            </div>
            <div class="u-container-style u-custom-item u-list-item u-palette-5-light-3 u-repeater-item">
              <div class="u-container-layout u-similar-container u-valign-bottom-sm u-container-layout-3">
                <h3 class="u-text u-text-5"><b> DV-ConvNet: Fully Convolutional Deep Learning on Point Clouds with Dynamic Voxelization and 3D Group Convolution</b>
                </h3>
                <img class="u-hidden-md u-hidden-sm u-hidden-xs u-image u-image-default u-image-3" src="images/WX20220418-1235222x.png" alt="" data-image-width="632" data-image-height="516">
                <p class="u-text u-text-6"> 3D point cloud interpretation is a challenging task due
to the randomness and sparsity of the component points.
Many of the recently proposed methods like PointNet and
PointCNN have been focusing on learning shape descrip-
tions from point coordinates as point-wise input features,
which usually involves complicated network architectures.
In this work, we draw attention back to the standard 3D
convolutions towards an efficient 3D point cloud interpreta-
tion. Instead of converting the entire point cloud into voxel
representations like the other volumetric methods, we vox-
elize the sub-portions of the point cloud only at necessary
locations within each convolution layer on-the-fly, using
our dynamic voxelization operation with self-adaptive vox-
elization resolution. In addition, we incorporate 3D group
convolution into our dense convolution kernel implemen-
tation to further exploit the rotation invariant features of
point cloud. Benefiting from its simple fully-convolutional
architecture, our network is able to run and converge at a
considerably fast speed, while yields on-par or even better
performance compared with the state-of-the-art methods on
several benchmark datasets.&nbsp;</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    
    
    
    <footer class="u-align-center-md u-align-center-sm u-align-center-xs u-clearfix u-footer u-grey-80" id="sec-f544"><div class="u-clearfix u-sheet u-sheet-1">
        <p class="u-align-left u-text u-text-1"><b>CONTACT</b>
          <br>
          <span style="font-size: 0.75rem;"><b>Address: </b>&nbsp;
          </span>
          <span style="font-size: 0.75rem;">Room 3572, Academic Building, HKUST, Clear Water Bay, Kowloon, HKSAR<br>
          </span>
          <span style="font-size: 0.75rem;"><b>Email:</b> ceyhwang@ust.hk<br>
          </span>
          <span style="font-size: 0.75rem;"><b>Tel: </b>+852 2358 8757<br>
          </span>
          <span style="font-size: 0.75rem;"><b>Fax: </b>+852 2358 1534
          </span>
          <br>
        </p>
        <a href="https://nicepage.com" class="u-align-right u-image u-logo u-image-1">
          <img src="//clustrmaps.com/map_v2.png?cl=525252&amp;w=a&amp;t=tt&amp;d=FN4dwNSI24yRjn4zdgH06hyvA6GYaWtMHapqL9G6vzs&amp;co=f0f0f0&amp;ct=474747" class="u-logo-image u-logo-image-1">
        </a>
        <p class="u-align-left u-text u-text-2"><i> Copyright © 2022 Yu-Hsing WANG.&nbsp;All rights reserved.</i>
        </p>
      </div></footer>
  </body>
</html>